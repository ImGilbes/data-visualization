{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5787b4e",
   "metadata": {},
   "source": [
    "### PCA:\n",
    "\n",
    "Dimensionality reduction technique that aims at minimizing the information loss, where the information is given by the variance of the data on a certain direction.\n",
    "\n",
    "PCA creates a new set of variables |x_new| < |x_old| that are uncorrelated and encode the data along the first n principal components. Principal components represent the directions along which the data has the maximal variance.  As an example, the first component is the direction with most possible variance in the dataset, the second component the direction with the second highest variance, etc. The first n components form new axes on which the data in the original space can be projected.\n",
    "\n",
    "The procedure is the following:\n",
    "* Data needs to be standardized before applying PCA ( a x f )\n",
    "* Step 1: compute the covariance matrix ( f x f ) - you can use SVD or correlation matrix too\n",
    "* Step 2: computer eigenvectors of Cov(data). The eigenvectors represent the principal components and their eigenvalues are the variance of each component ( f x f )\n",
    "* Step 3: sort (descensind order) the eigenvectors by eigenvalue to obtain the first n principal components ( f x f )\n",
    "* Step 4: discard the last index > n principal components (eigenvectors) ( f x n )\n",
    "* Step 5: the new dataset is simply given by the dot product of the original standardized dataset with the n first principal components ( f x n )T ( a x f)T\n",
    "\n",
    "PCA is linear dimensionality reduction method because the dataset in the new feature space is the result of linear combinations of the original dataset (dot product)\n",
    "\n",
    "* Pros:\n",
    "    * PCA can be used for reducing noise since it removes those directions with little impact on the information of the data. \n",
    "    * It can be used as a pre processing technique (other than for removing noise, you can use it to simplify the dataset) - just remove a small number of dimensions like 10 > 8, or 100 > 90\n",
    "    * Allows to plot high-dimensiionality datasets (see cons)\n",
    "        \n",
    "* Cons:\n",
    "    * Does not keep information on the structure of the data, both locally and globally\n",
    "    * PCA is overshadowed by many other algorithms when it comes to data exploration tasks. It is hard to visualize data and its global structure - graph-based algorithms are better in that regard.\n",
    "    * Can only preserve linear relationships between variables\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c978d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4071baba",
   "metadata": {},
   "source": [
    "### MDS:\n",
    "\n",
    "MDS is a family of techniques that can be use for dimensionality reduction as well as a number of other tasks in which is it required to discover the hidden structure of data.\n",
    "\n",
    "The goal of MDS algorithms is to find a representation of data in lower-dimensional spaces that preserves similarities ( = distance) between data points.\n",
    "\n",
    "MDS algorithms are classified in two main categories: metric and non-metric. The first type assumes that the distance between data points in the lower-dimensional space is a function of the similarity in the higher-dimensional space. Non-metric MDS relaxes this definition, and assumes that pairwise similarities keep their order once scaled to lower-dim. This means that if d(a,b) > d(b,c) in the original feature space, that has to be true also in the target feature space.\n",
    "\n",
    "In the classic algorithm (there are many modern/updated variations) the precedure is the following:\n",
    "* Construct a pairwise similarity matrix (can use euclidean distance)\n",
    "* Center the similarity matrix\n",
    "\n",
    "\n",
    "* Pros:\n",
    "    * Because it preserves similarities, MDS is great for data such as correlations, ratings (with the same scale) , or pairwise distances - in general data that embed on their own the notion of similarity.\n",
    "    * In general it preserves pairwise relationships (it was created for that)\n",
    "    * It can encode non-linear relationships between variables (PCA cannot, it's a linear technique)\n",
    "* Cons:\n",
    "    * It's of little use for visualizing densely populated datasets, like MNIST.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a2ea5",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "\n",
    "\n",
    "t-SNE aims at replicating and approximating the distribution of the pairwise distances (= similarities) in the starting feature space with a Student t distribution in the arrival feature space. This is possible by optimizing the parameters of the t distribution using as cost function the Kullback-Leibler (KL) divergence, which measures the difference between two probability distributions. The problem is thus turned into an optimization problem which is solved through gradient descent.\n",
    "\n",
    "The pairwise distances in the starting feature spaces are assumed to be distributed like a gaussian and are computed with the gaussian kernel (RBF). Instead, the t distribution used in the arrival space helps in handling outliers (not influenced as much as the gaussian because of the heavy tail).\n",
    "\n",
    "One of the drawbacks of the using KL div and t-distr. is that there is a big cost (or error, loss) for representing originally close points as distant points, whereas there is a small cost for representing originally distant points as close. This issue can be overcome by forcing the data to stay close at the beginning (early compression), dissimilar points will then repel each other.\n",
    "\n",
    "t-SNE takes as input an hyperparameter called perplexity which regulates the trade-off between the focus on local vs global structure. Perplexity is used to determine the number of neighbours in the high-dimensional space, thus it shrinks or increases the size of the (gaussian shaped/decaying) neighbourhoods. The higher the perplexity the more neighbours a point has in the satrting feature space. The results of t-sne are heavily dependent on the configuration of perplexity, so it is recommended to try multiple configuration per dataset. An appropriate choice of perplexity is usually between 5 and 50 but there is no rule on thta, just heuristics.\n",
    "\n",
    "\n",
    "\n",
    "Steps:\n",
    "* Compute the matrix of pairwise simiilarities with the gaussian kernel (RBF)\n",
    "* Iteratively adjust the new position of points in order to optimize the KL divergence between the pairwise distributions in the starting and arrival feature space\n",
    "\n",
    "\n",
    "* Pros:\n",
    "    * Good preservation of global and local structures\n",
    "    * Clusters are well highlighted, especially in comparison with PCA\n",
    "    * Not as sensible to outliers as PCA or MDS.\n",
    "* Cons:\n",
    "    * **Very** computationally expensive, especially when compared to UMAP\n",
    "    * Very dependent on the configuration of the hyperparameter -> perplexity, it is best to always plot the same dataset multiple times with different perplexities.\n",
    "    * Can be outclassed by UMAP when trying to analyze the structure of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2228ac",
   "metadata": {},
   "source": [
    "### UMAP\n",
    "UMAP is a graph-based algorithm that aims at replicating the graph of points in the starting feature space in the arrival space. The higher-dimensionality space UMAP constructs a weighted neighbour graph (specifically a fuzzy simplicial complex, where the weights represent the probability that two edges are connected). In the lower dimensionality the graph is created with new weights which are estimated by optimizing the cross-entropy between the original weights wh and the new ones wl. The problem is thus turned into an optimization problem that can be solved with gradient descent.\n",
    "\n",
    "Also UMAP, like t-SNE, is very dependent on the choice of hyperparameters. Namely, on the number of neightbours n_neighbours that should be considered when building the high-dimensional graph. A small n_neighbours can show more separate, sparse clusters, highlighting the division between them (remember that in the arrival space the distance between clusters may not have a meaning, only the fact that they are separated is meaningful). Higher choice of n_neighbours can help showing the global structure of data, but also leads to more dense neighbourhoods and clusters, potentially comrpessing points together. A second hyperparameter, min_dist, comes in help in this case. min_dist defines the minimum spacing between points in the arrival feature space. High min_dist explodes the structure of clusters, creating big spaces between points of teh same cluster. On the other hand, lower min_dist allows points to get close together and create tight clusters, thus helping in visualizing the global structure of data.\n",
    "\n",
    "At the following link https://pair-code.github.io/understanding-umap/ Figure 7 gives an example in which t-SNE captures the global structure of the data, whereas UMAP cannot. The two algorithms should be used complementarily and their results should be compared to a√¨have a better overview of the data.\n",
    "\n",
    "Steps:\n",
    "* Find the nearest neighbours (distance metric of choice, can use euclidean) of each datapoint based on the n_neighbours parameter and create a neighbour graph.\n",
    "* Assign a weight to each edge based on the similarity of each point\n",
    "\n",
    "* Pros:\n",
    "    * great global and local structure preservation. Suitable for data explorationa and visualization\n",
    "    * incredibly faster that other techniques that have the same presion for local-global structure preservation\n",
    "* Cons:\n",
    "    * very dependent on its hyperparameters, data should always be plotted with multiple configurations\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5dc58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe494567",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c089b980",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3579c734",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cc6e080",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "852968d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e80922da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fc697f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcd61c01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d458439e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b82e539",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
